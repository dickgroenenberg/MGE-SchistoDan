import os
import csv
import yaml
from snakemake.io import expand
import pandas as pd
import glob
import sys
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
            




print("Configuration loaded:", config)


#Parse samples from .csv files
def parse_samples(samples_file):
    samples = {}
    with open(samples_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['ID']
            forward_read = row['forward']
            reverse_read = row['reverse']
            samples[sample_id] = {"R1": forward_read, "R2": reverse_read}
    return samples




#Parse references from CSV
def parse_sequence_references(sequence_reference_file):
    sequence_references = {}
    with open(sequence_reference_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['process_id']
            reference_name = row['reference_name']
            protein_reference_path = row['protein_reference_path']
            nucleotide_reference_path = row['nucleotide_reference_path']
            sequence_references[sample_id] = {
                "reference_name": reference_name, 
                "protein_path": protein_reference_path,
                "nucleotide_path": nucleotide_reference_path
            }    
    return sequence_references


print("Loaded config keys:", config.keys())

samples = parse_samples(config["samples_file"])
sequence_references = parse_sequence_references(config["sequence_reference_file"])

output_dir = config["output_dir"]

r = config["r"]
s = config["s"]

run_name = config["run_name"]

mge_path = config["mge_path"]

preprocessing_mode = config.get("preprocessing_mode", "concat")  #Default to 'concat'/'standard' mode if not specified





#Create lists of samples and corresponding references, and link them
sample_list = list(samples.keys())

reference_list = [sequence_references[sample]["reference_name"] for sample in sample_list]

sample_reference_pairs = list(zip(sample_list, reference_list))


print(f"Run name: {run_name}")
print(f"Output directory: {output_dir}")
print(f"r params: {r}")
print(f"s params: {s}")
print("Sample files:", samples)
print("Sequence references:", sequence_references)
print("reference list:", reference_list)
print("sample reference pairs:", sample_reference_pairs)






#Rule them ALL
rule all:
    input:
        expand(
            os.path.join(output_dir, "consensus/{sample}_r_{r}_s_{s}_con_renamed_{reference_name}.fas"),
            zip,
            sample=[s for s, _ in sample_reference_pairs],
            reference_name=[ref for _, ref in sample_reference_pairs],
            r=config["r"],
            s=config["s"]
        ),
        os.path.join(output_dir, f"consensus/{run_name}.fasta"),
        os.path.join(output_dir, "alignment/alignment_files.log"),
        os.path.join(output_dir, f"{run_name}.csv"), 
        os.path.join(output_dir, "cleanup_complete.txt")




# Preprocessing rules for merge mode
if preprocessing_mode == "merge":
    rule fastp_pe_merge:
        input:
            R1=lambda wildcards: samples[wildcards.sample]["R1"],
            R2=lambda wildcards: samples[wildcards.sample]["R2"]
        output:
            R1_trimmed=os.path.join(output_dir, "trimmed_data/{sample}_R1_trimmed.fq.gz"),
            R2_trimmed=os.path.join(output_dir, "trimmed_data/{sample}_R2_trimmed.fq.gz"),
            report=os.path.join(output_dir, "trimmed_data/reports/{sample}_fastp_report.html"),
            json=os.path.join(output_dir, "trimmed_data/reports/{sample}_fastp_report.json"),
            merged_reads=temp(os.path.join(output_dir, "trimmed_data/{sample}_merged.fq")),
            unpaired_R1=temp(os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R1.fq")),
            unpaired_R2=temp(os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R2.fq"))
        log:
            err=os.path.join(output_dir, "trimmed_data/{sample}_fastp.err")
        shell:
            """
            fastp -i {input.R1} -I {input.R2} \
                  -o {output.R1_trimmed} -O {output.R2_trimmed} \
                  --adapter_sequence=AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \
                  --adapter_sequence_r2=AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \
                  --dedup \
                  --trim_poly_g \
                  --merge --merged_out {output.merged_reads} \
                  --unpaired1 {output.unpaired_R1} \
                  --unpaired2 {output.unpaired_R2} \
                  -h {output.report} -j {output.json} \
                  > {log.err}
            """

    rule clean_headers_merge:
        input:
            merged_reads=os.path.join(output_dir, "trimmed_data/{sample}_merged.fq")
        output:
            clean_merged=os.path.join(output_dir, "trimmed_data/{sample}_merged_clean.fq")
        shell:
            """
            sed 's/ /_/g' {input.merged_reads} > {output.clean_merged}
            """

    # Define input for MGE based on merge mode
    def get_mge_input(wildcards):
        return os.path.join(output_dir, f"trimmed_data/{wildcards.sample}_merged_clean.fq")

else:  # concat mode
    rule gunzip_and_clean_headers:
        input:
            R1=lambda wildcards: samples[wildcards.sample]["R1"],
            R2=lambda wildcards: samples[wildcards.sample]["R2"]
        output:
            R1_out=temp(os.path.join(output_dir, "raw_data/{sample}_R1_clean.fastq")),
            R2_out=temp(os.path.join(output_dir, "raw_data/{sample}_R2_clean.fastq"))
        shell:
            """
            if [[ {input.R1} == *.gz ]]; then
                pigz -cd {input.R1} > {output.R1_out}
            else
                cp {input.R1} {output.R1_out}
            fi

            if [[ {input.R2} == *.gz ]]; then
                pigz -cd {input.R2} > {output.R2_out}
            else
                cp {input.R2} {output.R2_out}
            fi
            
            sed -i 's/ /_/g' {output.R1_out}
            sed -i 's/ /_/g' {output.R2_out}
            """

    rule fastp_pe_concat:
        input:
            R1=os.path.join(output_dir, "raw_data/{sample}_R1_clean.fastq"),
            R2=os.path.join(output_dir, "raw_data/{sample}_R2_clean.fastq")
        output:
            R1_trimmed=os.path.join(output_dir, "trimmed_data/{sample}_R1_trimmed.fastq"),
            R2_trimmed=os.path.join(output_dir, "trimmed_data/{sample}_R2_trimmed.fastq"),
            report=os.path.join(output_dir, "trimmed_data/reports/{sample}_fastp_report.html"),
            json=os.path.join(output_dir, "trimmed_data/reports/{sample}_fastp_report.json")
        log:
            out=os.path.join(output_dir, "trimmed_data/{sample}_fastp.out"),
            err=os.path.join(output_dir, "trimmed_data/{sample}_fastp.err")
        shell:
            """
            fastp -i {input.R1} -I {input.R2} \
                  -o {output.R1_trimmed} -O {output.R2_trimmed} \
                  -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \
                  -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \
                  --dedup \
                  --trim_poly_g \
                  -h {output.report} -j {output.json} \
                  > {log.out} 2> {log.err}
            """

    rule fastq_concat:
        input:
            R1=os.path.join(output_dir, "trimmed_data/{sample}_R1_trimmed.fastq"),
            R2=os.path.join(output_dir, "trimmed_data/{sample}_R2_trimmed.fastq")
        output:
            temp(os.path.join(output_dir, "trimmed_data/{sample}_concat.fastq"))
        shell:
            """
            cat {input.R1} {input.R2} > {output}
            """

    rule quality_trim:
        input:
            os.path.join(output_dir, "trimmed_data/{sample}_concat.fastq")
        output:
            temp(os.path.join(output_dir, "trimmed_data/{sample}_concat_trimmed.fq")),
            report=os.path.join(output_dir, "trimmed_data/reports/{sample}_concat.fastq_trimming_report.txt")
        threads: 4
        retries: 3 
        shell:
            """
            trim_galore --cores {threads} --dont_gzip --output_dir {output_dir}/trimmed_data/ {input}
            """

    # Define input for MGE based on concat mode
    def get_mge_input(wildcards):
        return os.path.join(output_dir, f"trimmed_data/{wildcards.sample}_concat_trimmed.fq")




#Common rules for both preprocessing routes below
#MGE rule
rule MitoGeneExtractor:
    input:
        DNA=get_mge_input,
        AA=lambda wildcards: sequence_references[wildcards.sample]["protein_path"]
    output:
        alignment=os.path.join(output_dir, "alignment/{sample}_r_{r}_s_{s}_align_{reference_name}.fas"),
        consensus=os.path.join(output_dir, "consensus/{sample}_r_{r}_s_{s}_con_{reference_name}.fas"),
        vulgar=os.path.join(output_dir, "logs/{sample}_vulgar_r_{r}_s_{s}_{reference_name}.txt")
    log:
        out=os.path.join(output_dir, "out/{sample}_r_{r}_s_{s}_summary_{reference_name}.out"),
        err=os.path.join(output_dir, "err/{sample}_r_{r}_s_{s}_summary_{reference_name}.err")
    params:
        mge_executor=config["mge_path"]
    shell:
        """
        #echo input.AA list to log
        echo "Input AA files: {input.AA}" > {log.out}

        {params.mge_executor} \
        -q {input.DNA} -p {input.AA} \
        -o {output_dir}/alignment/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_align_ \
        -c {output_dir}/consensus/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_con_ \
        -V {output.vulgar}  \
        -r {wildcards.r} -s {wildcards.s} \
        -n 0 -C 5 -t 0.5 \
        > {log.out} 2> {log.err}
        """




#Define list of consensus files and renamed consensus files for downstream rules
consensus_files = [
    os.path.join(output_dir, f"consensus/{sample}_r_{r}_s_{s}_con_{reference_name}.fas")
    for sample, reference_name in sample_reference_pairs
    for r in config["r"]
    for s in config["s"]
]
print(f"Consensus files: {consensus_files}")





renamed_files = [
    os.path.join(output_dir, f"consensus/{sample}_r_{r}_s_{s}_con_renamed_{reference_name}.fas")
    for sample, reference_name in sample_reference_pairs
    for r in config["r"]
    for s in config["s"]
]
print(f"Renamed consensus files: {renamed_files}")





#Rename headers in consensus files
if preprocessing_mode == "merge":
    rule rename_fasta_headers:
        input:
            consensus_files=consensus_files
        output:
            renamed_consensus_files=renamed_files
        threads: workflow.cores
        run:
            # Progress tracking variables
            total_files = len(input.consensus_files)
            processed_files = 0
            progress_lock = Lock()
            last_reported_progress = 0

            def report_progress():
                nonlocal processed_files, last_reported_progress
                with progress_lock:
                    processed_files += 1
                    progress_percentage = (processed_files * 100) // total_files
                    
                    # Report every 10% progress
                    if progress_percentage >= last_reported_progress + 10:
                        print(f"Progress: {processed_files}/{total_files} files processed ({progress_percentage}%)")
                        last_reported_progress = (progress_percentage // 10) * 10

            def process_file(file_pair):
                try:
                    input_file, output_file = file_pair
                    base_name = os.path.basename(input_file)
                    
                    parts = base_name.split('_')
                    sample = parts[0]
                    r = parts[2]
                    s = parts[4]
                    con_suffix = base_name.split('con_')[-1].replace('.fas', '')
                    
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    
                    sequence_count = 0
                    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:
                        for line in infile:
                            if line.startswith('>'):
                                sequence_count += 1
                                new_header = f">{sample}_r_{r}_s_{s}_{con_suffix}_merge\n"
                                outfile.write(new_header)
                            else:
                                outfile.write(line)
                    
                    report_progress()
                    return True
                    
                except Exception as e:
                    print(f"Error processing {input_file}: {str(e)}", file=sys.stderr)
                    raise

            # Log rule start
            print(f"Starting rename_fasta_headers rule with {total_files} files")
            print(f"Using {threads} threads")
            
            # Check input files exist
            missing_files = [f for f in input.consensus_files if not os.path.exists(f)]
            if missing_files:
                error_msg = f"Missing input files: {', '.join(missing_files)}"
                print(error_msg, file=sys.stderr)
                raise FileNotFoundError(error_msg)

            # Create pairs of input and output files
            file_pairs = list(zip(input.consensus_files, output.renamed_consensus_files))
            
            # Process files in parallel with timing
            start_time = datetime.now()
            try:
                with ThreadPoolExecutor(max_workers=threads) as executor:
                    results = list(executor.map(process_file, file_pairs))
                    
                # Log completion statistics
                end_time = datetime.now()
                duration = end_time - start_time
                successful = sum(results)
                
                print(f"Rename operation completed in {duration}")
                print(f"Successfully processed {successful} out of {total_files} files")
                
            except Exception as e:
                print(f"Failed to complete rename operation: {str(e)}", file=sys.stderr)
                raise

else:  # concat mode
    rule rename_fasta_headers:
        input:
            consensus_files=consensus_files
        output:
            renamed_consensus_files=renamed_files
        threads: workflow.cores
        run:
            # Progress tracking variables
            total_files = len(input.consensus_files)
            processed_files = 0
            progress_lock = Lock()
            last_reported_progress = 0

            def report_progress():
                nonlocal processed_files, last_reported_progress
                with progress_lock:
                    processed_files += 1
                    progress_percentage = (processed_files * 100) // total_files
                    
                    # Report every 10% progress
                    if progress_percentage >= last_reported_progress + 10:
                        print(f"Progress: {processed_files}/{total_files} files processed ({progress_percentage}%)")
                        last_reported_progress = (progress_percentage // 10) * 10

            def process_file(file_pair):
                try:
                    input_file, output_file = file_pair
                    base_name = os.path.basename(input_file)
                    
                    parts = base_name.split('_')
                    sample = parts[0]
                    r = parts[2]
                    s = parts[4]
                    con_suffix = base_name.split('con_')[-1].replace('.fas', '')
                    
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    
                    sequence_count = 0
                    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:
                        for line in infile:
                            if line.startswith('>'):
                                sequence_count += 1
                                new_header = f">{sample}_r_{r}_s_{s}_{con_suffix}\n"
                                outfile.write(new_header)
                            else:
                                outfile.write(line)
                    
                    report_progress()
                    return True
                    
                except Exception as e:
                    print(f"Error processing {input_file}: {str(e)}", file=sys.stderr)
                    raise

            # Log rule start
            print(f"Starting rename_fasta_headers rule with {total_files} files")
            print(f"Using {threads} threads")
            
            # Check input files exist
            missing_files = [f for f in input.consensus_files if not os.path.exists(f)]
            if missing_files:
                error_msg = f"Missing input files: {', '.join(missing_files)}"
                print(error_msg, file=sys.stderr)
                raise FileNotFoundError(error_msg)

            # Create pairs of input and output files
            file_pairs = list(zip(input.consensus_files, output.renamed_consensus_files))
            
            # Process files in parallel with timing
            start_time = datetime.now()
            try:
                with ThreadPoolExecutor(max_workers=threads) as executor:
                    results = list(executor.map(process_file, file_pairs))
                    
                # Log completion statistics
                end_time = datetime.now()
                duration = end_time - start_time
                successful = sum(results)
                
                print(f"Rename operation completed in {duration}")
                print(f"Successfully processed {successful} out of {total_files} files")
                
            except Exception as e:
                print(f"Failed to complete rename operation: {str(e)}", file=sys.stderr)
                raise



#Concatenate all consensus fasta files
rule concatenate_fasta:
    input:
        renamed_consensus=lambda wildcards: [
            os.path.join(output_dir, f"consensus/{sample}_r_{r}_s_{s}_con_renamed_{reference_name}.fas")
            for sample, reference_name in sample_reference_pairs
            for r in config["r"]
            for s in config["s"]
        ]
    output:
        os.path.join(output_dir, f"consensus/{run_name}.fasta")
    threads: workflow.cores
    shell:
        """
        parallel --jobs {threads} 'cat {{}}' ::: {input.renamed_consensus} > {output}
        """




#Create list of alignment files for stats
rule create_alignment_log:
    input:
        alignment_files=lambda wildcards: glob.glob(os.path.join(output_dir, "alignment/*_align_*.fas")),
        concat_cons=os.path.join(output_dir, f"consensus/{run_name}.fasta")
    output:
        alignment_log=os.path.join(output_dir, "alignment/alignment_files.log")
    run:
        alignment_files = input.alignment_files

        with open(output.alignment_log, 'w') as log_file:
            for file in alignment_files:
                log_file.write(f"{file}\n")





#Extract stats from alignment files using custom python script
rule extract_stats_to_csv:
    input:
        alignment_log=os.path.join(output_dir, "alignment/alignment_files.log"),
        out_files=expand(os.path.join(output_dir, "out/{sample}_r_{r}_s_{s}_summary_{reference_name}.out"),
                         zip, 
                         sample=[s for s, _ in sample_reference_pairs],
                         reference_name=[ref for _, ref in sample_reference_pairs],
                         r=config["r"],
                         s=config["s"]),
        concat_cons=os.path.join(output_dir, f"consensus/{run_name}.fasta")  
    output:
        stats=os.path.join(output_dir, "{run_name}.csv")
    params:
        script="./scripts/mge_stats.py",
        out_dir=os.path.join(output_dir, "out/")
    shell:
        """
        python {params.script} {input.alignment_log} {output.stats} {params.out_dir}
        """




#Clean up superfluous files
rule cleanup_files:
    input:
        summary_csv=os.path.join(output_dir, f"{run_name}.csv")
    output:
        touch(os.path.join(output_dir, "cleanup_complete.txt"))
    run:
        #Remove all .log files in the logs directory
        log_dir = os.path.join(output_dir, "logs")

        log_files = glob.glob(os.path.join(log_dir, "*.log"))
        if log_files:
            for log_file in log_files:
                print(f"Removing log file: {log_file}")
                os.remove(log_file)
        
        with open(output[0], 'w') as f:
            f.write("Cleanup complete.")
